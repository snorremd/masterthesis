%!TEX root = ../Thesis.tex
% Chapter Template

\chapter{Methodology} % Main chapter title

\label{Methodology} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref{Methodology}. \emph{Methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\color{red}TODO: The methodology and testing chapters are somewhat intertwined. The incrementally optimized parameter set is listed under algorithmic design in the methodology chapter, but how this parameter set is derived is not explained fully before the testing and evaluation chapter. Perhaps the incremental test section in the test chapter should be moved to the pre-experimental planning section in this methodology chapter.

The methodology chapter should be written in past tense?

\color{black}

Research methodologies and standards used to test retrieval and classification algorithms form the foundation of this thesis' research methodology. While experiments in the information retrieval field do not necessarily directly involve human subjects, there are still standards and methodologies in place to ensure that experimental results are valid. This chapter will describe the scientific approach used in the scientific field of information retrieval, and how this approach was applied to this thesis work.

\section{Experimental Evaluation}
\label{ExperimentalEvaluation}
The configuration and parameter sets for the \CTC algorithm was evaluated experimentally rather than analytically for reasons explained by \cite[][32]{Sebastiani2002}:
\begin{quote}The reason is that, in order to evaluate a system analytically (e.g., proving that the system is correct and complete), we would need a formal specification of the problem that the system is trying to solve (e.g., with respect to what correctness and completeness are defined), and the central notion of TC [Text Classification] (namely, that of membership of a document in a category) is, due to its subjective character, inherently nonformalizable. The experimental evaluation of a classifier usually measures its effectiveness (rather than its efficiency), that is, its ability to take the right classification decisions.
\end{quote}

Experimental evaluation have long been used and discussed in the field of information retrieval. One of the first experimental paradigms in information retrieval research, one that is still in use today, is the test collection evaluation paradigm introduced by The Cranfield research projects during the 60s \cite{Cleverdon1966}. The experimental methodology formed during these experiments are nicely summarized by \cite[][564]{Voorhees2005} who writes that:

\begin{quote}
At the core of this experimental methodology was the idea that live users could be removed from the evaluation loop, thus simplifying the evaluation and allowing researchers to run in vitro–style experiments in a laboratory with just their retrieval engine, a set of queries, a test collection, and a set of judgments (i.e., a list of relevant documents).
\end{quote}.

\cite[p. 33]{Cleverdon1966} give three requirements for using measurements of performance in experimental tests of information retrieval systems:
\begin{enumerate}
\item A document collection of known size to be used in the test;
\item A set of questions, together with decisions as to exactly which documents are relevant to each question;
\item A set of results of searches made in the test; these usually give the numbers of documents retrieved in the searches, divided into the relevant and non-relevant documents.
\end{enumerate}
These questions should be asked with regards to information retrieval experiments done on text search via queries, but can inspire similar questions for experiments done on text classification and clustering algorithms. Instead of forming questions and determining which documents are relevant to those question, one can form a set of categories and then decide which documents fall into which categories. Then clustering results can be divided into clusters, each cluster correct or incorrect.

\section{Corpora}
\label{Corpora}

When performing experimental research on information retrieval systems it is customary to use standard document collections or corpora as they are also known. There are several corpora available for text classification and clustering research. \cite{Baeza-Yates2011a} describe some of the corpora available for text classification research among them: Reuters-21578, RCV: Reuters Corpus Volumes, the OHSUMED reference collection and 20 NewsGroups. Some of them are briefly explained below.

Reuters, an international news agency have made several corpora that are available trough different sources. One Reuters corpus that have been much used in the text classification community is the \textit{Reuters 21578} corpus \cite{Lewis2004a}. The documents in this collection was collected from documents appearing on the Reuters newswire in 1987. The corpus was assembled and categorized by personnel from Reuters and Carnegie Group in 1987. This corpus thus resembles that used in the ``Recycling of news in the news papers 1999 - 2000'' research project.

Reuters have since made a new corpus that is likely to replace the Reuters 21578 corpus. This new corpus is divided into three volumes RCV1, RCV2 and TRC2. The first two volumes contain news stories from 96 - 97, and the last volume contains news stories from 08 to 09. RCV1 and TRC2 contain English news stories only, while RCV2 is multilingual \cite{NationalInstituteofStandardsandTechnology2004}. An article on use of the RCV1 corpus provide more details about how the data set can be used in evaluation text categorization systems. ``\textit{Reuters CorpusVolume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced.}'' \cite{Lewis2004}. 

This thesis focus mainly on news corpora, but the research group investigating the ``Klimauken'' corpus has also looked into the blogosphere. With this in mind it could be interesting to use a standard blog collection for evaluation of the algorithm in future research. Two blog collections are used in the blog track in the TREC conference, the Blogs06 and Blogs08 collections.\begin{quote}
The TREC Blogs06 collection is a big sample of the blogosphere, and contains spam as well as possibly non-blogs, e.g. RSS feeds from news broadcasters. It was crawled over an eleven week period from 6th December 2005 until the 21st February 2006. The collection is 148GB in size [\dots] The collection was used in TREC 2006, 2007 and 2008 \cite{Macdonald2011}.
\end{quote} 

These corpora form a solid foundation for experimental evaluations and make it possible to replicate and compare results between research projects and researchers. But for this to be possible, it is necessary to use some formal evaluation measures that are employed by a majority of the research in the area of study. This will be the focus of the next section (Section~\ref{EvaluationMeasures}).

\section{Evaluation Measures}
\label{EvaluationMeasures}
There are some considerations to take when choosing evaluation measures. When performing experimental research it is important to use the same evaluation measures as those used in related research works to make the results comparable. In much of information retrieval research, text classification included, there are agreed upon measures that can be used while performing research. Such measures do not exist to the same extent for clustering algorithms because the measure of relevance may not be similar in different clustering research. For the \STC algorithm there seems to be a community of practice with regards to evaluation metrics. The measure of relevance in this research is quite similar to that used in much classification work where each document is assigned a class.

\cite{Chim2007} detail how they perform an experimental evaluation of their clustering result in some detail. \citeauthor{Chim2007} use the evaluation measures on the results from a hierarchical agglomerative clustering algorithm as well as the \STC algorithm. These formulas have also been used by \cite{Rafi2011} when they compare the standard suffix tree clustering algorithm of \citeauthor{Oren1998} with the algorithm formulated by \citeauthor{Chim2008}. Their papers describe four standard measurements for clustering quality: precision, recall, F-Measure and overall F-Measure.

If you have the sets

\begin{displaymath}
C = \{C_{1}, C_{2}, \dots, C_{k}\}
\end{displaymath}
\begin{displaymath}
C^* = \{C_1^*, C_2^*, \dots, C_l^*\}
\end{displaymath}
\begin{displaymath}
D = \{D_{1}, D_{2}, \dots, D_{k}\}
\end{displaymath}

where \(C\) is the clusters produced by the algorithms on document set \(D\), and \(C^*\) is the ``correct'' classes of document set \(D\), then the recall, precision and F-measure of cluster \(j\) with respect to class \(i\) can be calculated as:

\begin{displaymath}
recall(i,j) = \frac{\vert C_{j} \cap C_i^* \vert}{\vert C_i^* \vert}
\end{displaymath}
\begin{displaymath}
precision(i,j) = \frac{\vert C_{j} \cap C_i^* \vert}{\vert C_{j} \vert}
\end{displaymath}
\begin{displaymath}
F-Measure(i,j) = \frac{2 \cdot precision(i,j) \cdot recall(i,j)}{precision(i,j) + recall(i,j)}
\end{displaymath}

\cite{Chim2007} do not provide any information as to whether the classes of \(C^*\) must be disjoint (i.e. whether one document can be assigned to multiple classes). The authors use this performance measure on both the \STC and Hierarchical Agglomerative Clustering algorithms. The \STC algorithm allows clusters to be overlapping, i.e. a document can occur in more than one document. The Hierarchical Agglomerative algorithm builds non-overlapping clusters. Based on this information one can assume that the authors have used the above performance measurements on both the overlapping and non-overlapping cluster sets.

As was explained in the literature section recall aims to capture the fraction of positive results to the total number of correct results. In this context recall expresses the fraction of a category's documents the cluster contains. Precision shows the fraction of documents in a cluster that is correctly clustered given a category to the amount of documents in a cluster. Because precision and recall is not good measures by themselves (recall could be a 100\%, but often precision would then be very low) an F-Measure is often used in evaluation of text classifiers \cite{Baeza-Yates2011a}. The F-Measure is the harmonic mean between recall and precision and is high when both precision and recall is high \cite{Baeza-Yates2011b}. 

The precision, recall, and F-Measure measurements defined above are applied to one cluster and class at a time. In other words the F-Measure of a cluster j with regards to a class i might not be any good, but its F-Measure with regards to another class i' might be very good. \cite{Chim2007} define an overall F-Measure function that captures the F-Measures for all the correct classes defined for the document set. For this function only those clusters j which maximize the F-Measure score for class i are considered in the overall F-Measure score. The overall F-Measure is calculated using the function:

Given the formulas the overall F-Measure of the clusters \(C\) can be calculated using the formula:
\begin{displaymath}
F := \sum_{i=1}^{l} \frac{\vert C_i^* \vert}{\vert D \vert} \cdot \max_{j=1,\dots,k} \{F-Measure(i,j)\}
\end{displaymath}

\subsection{News articles project}
The ``Klimauken'' corpus is tagged with five topics (tags or classes) per document rather than a single topic. See the tags attribute in Listing~\ref{lst:snippetfile} for an example of a document in snippet format. In context of this corpus, a ground truth cluster is defined as all those documents that have all five topics in common. But only looking at perfect matches might exclude those clusters that closely match a ground truth cluster. It thus makes sense to adjust the relevance definition and talk about degrees of correctness. To this end a measure, discrepancy, is introduced. The discrepancy of a cluster \(C\) with regards to a ground truth cluster \(G\) is defined as \(5 - d\) where \(d\) is the number of tags in common in the set \(C_{tags} \cup G_{tags}\). If all documents in the cluster and ground truth cluster have all five topics in common, there is no discrepancy. Further we impose a limit on discrepancy, namely that \(C\) matches ground truth with discrepancy \(d\) iff there is a \(G\) such that:

\begin{displaymath}
\{ G \vert G \subseteq C \textit{ and } G = max\{d(G_{1}), \dots, d(G_{n})\}\}
\end{displaymath}

The discrepancy of a cluster in relation to the ground truth is thus its discrepancy to the best matching ground truth cluster. An example of a cluster is given in Table~\ref{tab:clusterexample}.

\subsubsection{Precision}
With a definition of discrepancy in place we understand precision to be the number of ground truth clusters of zero discrepancy divided by the number of retrieved clusters. The division of retrieved clusters into levels of discrepancy allows us to relax the original measure and for example allow ground truth clusters of discrepancy zero and one to be used when calculating precision. For a clustering result with many clusters of discrepancy 1, including these clusters when calculating the precision can have a big impact.

\subsubsection{Recall}

The recall of a clustering result \(C\), given ground truth clusters \(G\) and a discrepancy \(d\), can be calculated: \(Recall = \frac{\vert G_{d} \vert}{\vert C \vert}\). As with precision looking only at ground truth clusters with a discrepancy of zero gives us the recall value given a traditional relevance measurement. Again one can use the accumulated recall values of discrepancy levels zero and one to include mostly correct clusters as ground truth clusters.

\subsubsection{Tag Accuracy}
A third measure called tag accuracy can also be used to meaure the clustering result. This measure leaves behind the notion of ground truth all together in favor of measuring the coherency between tags in the documents in the resulting clusters. \textit{``A cluster has a tag accuracy of 5 - d if and only if d is the number of words common to all tags in it.''}, \cite{Moe2013}. Given two documents with the tags 
\begin{inparaenum}[\itshape 1\upshape)]
	\item Innenriks-ulykker-akeulykke-8åring-nordkapp, and
	\item Innenriks-kriminalitet-trafikk-førerkort-kristiansand,
\end{inparaenum}
we get a tag accuracy of 4.


\begin{table}[htdp]
\footnotesize
%\rowcolors{1}{gray!10}{white}
\begin{center}
\begin{tabular}{|c|p{10cm} |}
\hline
Label &  smører Odd-Bjørn \\
Topics overlap & 4 \\
Topics &  Nordmenniutland-sport-langrenn-hjelmeset-slovenia Nordmenniutland-sport-langrenn-hjelmeset-slovenia Nordmenniutland-sport-langrenn-verdenscup-slovenia \\
No. documents & 3 \\
Documents & www.vg.no/sport/ski/langrenn/artikkel.php?artid=581044 www.aftenposten.no/nyheter/sport/skisport/article3431048.ece www.adressa.no/sport/article1423419.ece \\
\hline
\end{tabular}
\end{center}
\caption{A cluster example}
\label{tab:clusterexample}
\end{table}


\begin{table}[htdp]
\footnotesize
%\rowcolors{1}{gray!10}{white}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Topic overlap &  Fraction of total clusters & Percentage  & accumulated\\ 
\hline
0 -precision & 253 / 457 & 0.554 & 0.554 \\
1 -precision & 2 / 457 & 0.004 & 0.558 \\
2 -precision & 1 / 457 & 0.002 & 0.560\\
3 -precision & 2 / 457 & 0.004 & 0.565\\
4 -precision & 8 / 457 & 0.018 & 0.582\\
5 -precision & 191 / 457 & 0.418 & 1.000\\
\hline
\end{tabular}
\\Total: 457 (of  457)
\end{center}
\caption{Precision of cluster result}
\label{tab:clusterprecision}
\end{table}


\section{Experimental Research}
\label{ExperimentalResearch}

This section will describe scientific experiments in context of algorithm analysis and outline an experiment design to test the hypotheses formulated to test the \CTC algorithm. The experimental design was formed around some of the techniques described in \cite{Bartz-Beielstein2004}, but with significant adjustments. In his article, \citeauthor{Bartz-Beielstein2004} is primarily concerned with the optimization of a particle swarm optimization algorithm. The techniques are non the less relevant as they provide valuable insight into experimental benchmarking of algorithms. The process is summarized in Table~\ref{tab:experimentsequence}.

\begin{table}[htdp]
\footnotesize
%\rowcolors{1}{gray!10}{white}
\begin{center}
\begin{tabular}{|c|p{10cm}|}
\hline
Step & Action\\
\hline
(S-1) & Pre–experimental planning\\
(S-2) & Scientific hypothesis\\
(S-3) & Statistical hypothesis\\
(S-4) & Specification\\
(a) & optimization problem,\\
(b) & constraints,\\
(c) & initialization method,\\
(d) & termination method,\\
(e) & algorithm (important factors),\\ 
(f) & initial experimental design,\\ 
(g) & performance measure\\
(S-5) & Experimentation\\
(S-6) & Statistical modeling of data and prediction\\
(S-7) & Evaluation and visualization\\
(S-8) & Optimization\\
(S-9) & Termination: If the obtained solution is good enough, or the maximum number of iterations has
been reached, go to step (S-11)\\
(S-10) & Design update and go to step (S-5)\\
(S-11) & Rejection/acceptance of the statistical hypothesis\\
(S-12) & Objective interpretation of the results from step (S-11)\\
\hline
\end{tabular}
\caption{``\textit{Sequential approach. This approach combines methods from computational statistics and exploratory data analysis to improve (tune) the performance of direct search algorithms.}'', from \protect \cite[p. 417]{Bartz-Beielstein2004}}
\label{tab:experimentsequence}
\end{center}
\end{table}

\subsection{Pre-experimental planning}
The first step is pre-experimental planning. During the pre-experimental planning tests were performed to find candidate parameters for the algorithm as well as good parameter ranges (see Chapter~\ref{EvaluationTesting}). Additionally the corpus file was processed to get a suitable format for clustering with different parameters. This included marking sentences up with appropriate text types.

\subsection{Scientific Hypotheses}

To investigate whether the optimization algorithm is effective a hypothesis was defined.


% \begin{description}
% 	\item []Optimal parameter set
% 	\begin{description}
% 	\item [\(H1_{1}\)] There is a parameter set \(p_{optimized}\) for the \CTC algorithm that for the ``Klimauken'' corpus give improved results compared to the average performance of 10 random parameter sets.
% 	\item [\(H1_{0}\)] There is no parameter set \(p_{optimized}\) that improves the performance of the \CTC algorithm for the ``klimauken'' corpus compared to the average performance of 10 random prameter sets.
% 	\end{description}
% \end{description}

\begin{description}
	\item []Optimization algorithm
	\begin{description}
	\item [\(H_{1}\)] The genetic algorithm gives values that for the ``klimauken'' corpus give better performance than the average performance of random values.
	% \item [\(H2_{1}\)] The genetic algorithm described in this thesis produce a parameter set \(p_{optimized}\) that for the ``klimauken'' corpus gives better performance than the average performance of ten random parameter sets.
	% \item [\(H2_{0}\)] The genetic algorithm described in this thesis does not produce a parameter set \(p_{optimized}\) that is worse than the default parameter set \(p_{default}\).
	\item [\(H_{0}\)] The genetic algorithm does not give values that for the ``klimauken'' corpus give better performance than the average performance of random values.
	\end{description}
\end{description}

The hypothesis only test the specific implementation of the Genetic Algorithm as implemented in this thesis work. Other optimization algorithms or other implementations of the Genetic Algorithm might perform differently from this one. By constricting the hypothesis to only account for this implementation of the Genetic Algorithm it is possible to formulate a null hypothesis. The results should however indicate whether this optimization approach has any virtue, or if genetic algorithms are wholly unsuitable for this optimization problem.

% \begin{description}
% 	\item []Genetic versus incremental search
% 	\begin{description}
% 	\item [\(H3_{1}\)] The genetic algorithm described in this thesis produce a parameter set \(p_{optimized}\) that is significantly better than an iteratively improved parameter set \(p_{iterativ}\).
% 	\item [\(H3_{0}\)] The genetic algorithm described in this thesis does not produce a parameter set \(p_{optimized}\) that is better than an iteratively improved parameter set \(p_{iterative}\).
% 	\end{description}
% \end{description}

The scope of this thesis does not allow for additional hypotheses to be investigated. For future research an additional possible hypothesis that would be worth testing is whether the optimized parameter set is also optimized for similar corpora.

\subsection{Specification of Experiment}
The specification of an experiment on the optimization of an algorithm should, according to \citeauthor{Bartz-Beielstein2004}, include a specification of the optimization problem, definition of constraints on the optimization, a description of how the algorithm is initialized, and an experiment design which describes the problem to be investigated and the algorithm design.


\subsubsection{Experiment Design}
The algorithm design describes the parameter sets used in the experimentation phase. The algorithm designs are listed in Table~\ref{tab:algorithmdesign}. Each column corresponds to one parameter in the algorithm and is labeled with an abbriviation of the parameter name. Please refer to Table~\ref{tab:parameterabbreviationdictionary} to see a dictionary for the abbriviations.

Three specific algorithm designs have been defined: \citeauthor{Oren1998} parameters, Genetic params, and \citeauthor{Moe2013} parameters. The \citeauthor{Oren1998} parameters are not directly relevant to the experiment. Its inclusion can be argued for because it serves as a interesting comparison for the average performance of the algorithm and the performance with optimized parameters. The \citeauthor{Moe2013} parameters were included on the same basis. These parameters measure how well the algorithm performs on a corpus when the parameters have been manually optimized for the ``Klimauken'' corpus. The last parameter set, Genetic params, were created using the \GA optimization process. It is this parameter set that will be used in the experiments. In addition to these three parameter sets, 100 random parameter sets were created to measure the average performance of the \CTC algorithm on the ``Klimauken'' corpus. This was done to establish a base performance to measure the optimized parameters against. These are not listed in Table~\ref{tab:algorithmdesign} for practical reasons.

The random parameters were generated using the generate\_random\_chromosome function implemented for the genetic algorithm. The spesific values and their limits are listed below:

\begin{description}
  \item[Tree types:] Suffix, Mid-gram, N-gram, Range-gram\hfill
  	\begin{description}
	  \item[Suffix:] No limits
	  \item[Mid-gram:] No limits
	  \item[N-gram:] 1 - 10
	  \item[Range-gram:] min: 0.0 - 0.9 \& max: 0.1 - 1.0
	\end{description}
  \item[Top base clusters amount:] 100 - 10 000
  \item[Min term occurrence:] 5 - 150
  \item[Max term ratio:] 0.05 - 1.00 (rounded to two decimals)
  \item[Min limit base cluster score:] 0 - 20
  \item[Max limit base cluster score:] 3 - 25 (if n > min limit base cluster score, min + 1 if not)
  \item[Drop singleton base clusters:] 0 or 1
  \item[Drop one word clusters:] 0 or 1
  \item[Order descending:] 0 or 1
  \item[Text types:] (0 - 1) for Frontpage Heading, Frontpage Introduction, Article Heading, Article Introduction, Article Byline, Article Text
  \item[Text amount:] 0.0 - 1.0
  \item[Similarity measure:] Etzioni, Jaccard, Cosine, Amendment 1C\hfill
  	\begin{description}
	  \item[Etzioni:] 0.0 - 1.0 (Etzioni threshold)
	  \item[Jaccard:] 0.0 - 1.0 (Jaccard threshold)
	  \item[Cosine:] 0.0 - 1.0 (Jaccard threshold) \& 0.0 - 1.0 (Cosine threshold)
	  \item[Amendment 1C:] 0.0 - 1.0 (Jaccard threshold), 5 - 500 (avg. corpus frequency threshold), \& 0 - 50 (intersect minimum limit) 
	\end{description}
\end{description}


\begin{landscape}
\begin{table}
\begin{center}
	\begin{tabular}{|l|l|l|}
	\hline
	Abbreviation & Full parameter name & Short explanation\\
	\hline
	TreeT & Tree type & The tree type (suffix, mid-gram, etc) used to build the phrase tree.\\
	TBC & Top Base Clusters & The top base clusters limit that determines the amount of base clusters to include.\\
	MinTO & Min term occurrence & The minimal occurrence of term in the corpus for it to not be considered a stop word.\\
	MaxTR & Max term ratio & The max allowed ratio of a term in the corpus for it not to be considered a stop word.\\
	MinLBCS & Min limit base cluster score & The min limit at which the base cluster label is scored the minimum score.\\
	MaxLBCS & Max limit base cluster score & The max limit at which the base cluster label is scored the maximum score.\\
	DSBC & Drop singleton base clusters & Whether to drop singleton base clusters or not.\\
	DOWC & Drop one word clusters & Whether to drop one word clusters or not.\\
	OD & Order descending & Whether to order the base clusters list in descending or ascending order.\\
	TA & Text amount & The amount of article text to include in the snippet expansion phase.\\
	TTy & Text types & Which kinds of text to include in the snippet expansion phase.\\
	SM & Similarity method & Which similarity method to use in the base clusters similarity function.\\
	\hline
	\end{tabular}
	\caption{A dictionary of abbreviations used in Table~\ref{tab:algorithmdesign}}
	\label{tab:parameterabbreviationdictionary}
\end{center}
\end{table}
\end{landscape}

\begin{landscape}
\begin{center}
\footnotesize
  \begin{longtable}{|p{1.5cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.2cm}|p{1.5cm}|p{0.8cm}|p{1cm}|p{2.5cm}|p{1.5cm}|}
  \hline
  Design & TreeT & TBC & MinTO & MaxTR & MinLBCS & MaxLBCS & DSBC & DOWC & OD & TA & TTy & SM\\
  \hline
  \endhead
  \citeauthor{Oren1998} parameters & Suffix & 500 & 4 & 0.4 & 2 & 7 & 0 & 0 & 1 & 0 & Frontpage Introduction, Frontpage Heading, Article Heading, Article Byline, Article Introduction & Etzioni (0.5 threshold)\\
  \hline
  Genetic params & 4-slice & 7937 & 141 & 0.47 & 11 & 17 & False & True & False & 0.32 & Front page intro, Article heading, article intro & Etzioni (0.99) \\
  \hline
  \citeauthor{Moe2013} parameters & Mid-gram & 5000 & 6 & 0.6 & 2 & 7 & 0 & 0 & 1 & 0 & Article Heading, Article Byline, Article Introduction & Etzioni (0.5 threshold)\\
  \hline
    \caption{Algorithm designs used in the experiments.}
	\label{tab:algorithmdesign}
  \end{longtable}
\end{center}
\end{landscape}

In \citetitle{Bartz-Beielstein2004} the problem design describes the optimization problems on which the particle swarm algorithm will work on. One can thus understand problem design as a definition of the problems that the algorithm is to solve. Table~\ref{tab:problemdesign} describes the clustering problem used in the experiment. The Experiment column specify the hypothesis for which the problem design was applied. The algorithm design is a reference to the parameter sets specified in Table~\ref{tab:algorithmdesign}. To keep the results in line with the classifications provided by the experts that prepared the ``Klimauken'' corpus, singleton ground truth clusters were preserved. The F-Beta constant was set to 1 to weigh precision and recall equally.

\begin{table}
\small
\begin{center}
  \begin{tabular}{|l|p{2.5cm}|l|l|l|}
  \hline
  Experiment & Algorithm Design & Drop singelton ground truth & F-Beta constant & Corpus\\ 
  \hline
  E1 & Genetic params & False & 1 & Klimauken\\
  \hline
  E1 & Random 1 & False & 1 & Klimauken\\
  \hline
  E1 & Random 2 & False & 1 & Klimauken\\
  \hline
  E1 & Random ... & False & 1 & Klimauken\\
  \hline
  E1 & Random 100 & False & 1 & Klimauken\\
  \hline
  \end{tabular}
\end{center}
\caption{The problem designs describing the clustering problem for the experiment.}
\label{tab:problemdesign}
\end{table}